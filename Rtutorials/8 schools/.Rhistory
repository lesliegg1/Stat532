mu.hat <- (sum((1/(sig.j^2 + tau^2))*y.j))/V.mu.inv
part1 <- -0.5*log(V.mu.inv)
part2 <- sum(-0.5*log(sig.j^2 + tau^2) -
0.5*((y.j - mu.hat)^2)/(sig.j^2 + tau^2))
out <- part1 + part2
return(out)
}
l.prior.t1 <- function(tau) { log(un.tfold(tau,df=1,s=5)) }
## Function to combine lik and prior pieces and exponentiate
lp.tauGy.t1 <- function(tau, y.j, sig.j) {lp.tauGy.lik(tau, y.j=y.j, sig.j=sig.j) + l.prior.t1(tau)}
p.tauGy.t1 <- function(tau) { exp(lp.tauGy.lik(tau, y.j=y.j, sig.j=sig.j) + l.prior.t1(tau)) }
## Function to obtain value proportional to log(p(tau|y)) = marg post of tau
## Using improper uniform prior on tau
l.prior.unif <- function(tau) {log(1)}
## Simple function to exponentiate results of above function
lp.tauGy.unif <- function(tau,y.j, sig.j) { lp.tauGy.lik(tau, y.j=y.j, sig.j=sig.j) + l.prior.unif(tau)}
p.tauGy.unif <- function(tau) { exp(lp.tauGy.lik(tau, y.j=y.j, sig.j=sig.j) + l.prior.unif(tau))}
### Let's look at the prior vs. marginal posterior for tau
### Plot p(tau|y) and p(tau) for the folded t and
### plot p(tau|y) for the uniform
grid.tau <- seq(0.001,50, length=5000) #play around with upper limit
#to see how things change
width <- grid.tau[2] - grid.tau[1]
grid.tau.g.y<- apply(cbind(grid.tau), 1, p.tauGy.t1)  #for folded-t prior
norm.grid.tau <- grid.tau.g.y/sum(grid.tau.g.y*width)
grid.tau.g.y2<- apply(cbind(grid.tau), 1, p.tauGy.unif) #unif prior on tau
norm.grid.tau2 <- grid.tau.g.y2/sum(grid.tau.g.y2*width)
#grid.tau.prior <- apply(cbind(grid.tau), 1, d.tfold, df=1, ncp=0)
grid.tau.prior <- apply(cbind(grid.tau), 1, un.tfold, df=1, s=5)
sum(grid.tau.prior*width)
norm.tau.prior <- grid.tau.prior/sum(grid.tau.prior*width) #grid version
plot(grid.tau, norm.grid.tau, type="n", xlim=c(0,50), ylim=c(0,0.22), xlab="tau",
ylab="density")
lines(grid.tau, norm.tau.prior, col=2, lwd=2, lty=1) #folded-t prior
lines(grid.tau, norm.grid.tau, col=3, lwd=2)  #marg. post with foldedt
lines(grid.tau, norm.grid.tau2, col=4, lwd=3, lty=1)
legend(15,0.16, legend=c("folded-t prior (df=1, s=5)","p(tau|y) (p(tau) = folded-t)",
"p(tau|y) (p(tau)= unif)"),lwd=c(2,2,2), lty=c(1,1,1), col=c(2,3,4), bty="n")
##Compare to Figure 5.5 in text
## NOTE: Values near zero are most plausible, zero is mode
## Values of tau larger than 10 are half as likely as near tau=0
## Be sure to look at and understand Figures 5.6 and 5.7
###  Question:  What might plausible values of tau be?
###  We already know upper limit on tau is about 100 from the context
###  of the problem. Also is helpful to stare at equations 5.17 and
###  5.20 to help think about what values of tau are reasonable.
###  What is tau the SD of?  How different could true SAT-coaching
###  effects at the school level really be?  Maybe 30 points?
###  this means that tau is probably not greater than 5? (15 +/- 3*5)
###  NOTE: For a classical analysis we could get an estimate of tau2 = MSB - MSW,
###  which can be negative!
### Now, let's think about where we can get into trouble with improper posteriors
### Since p(mu|tau,y) and p(theta|mu,tau,y) are both proper distributions (normals),
### if there is going to be a problem, it is going to be with p(tau|y)
### i.e. p(theta,mu,tau|y) has finite integral IFF p(tau|y) has finite integral
### What about p(mu, log(tau)) propto 1  (equivalent to p(mu,tau) propto 1/tau)?
l.prior.logunif <- function(tau) {log(1/tau)}
## Simple function to exponentiate results of above function
p.tauGy.logunif <- function(tau) { exp(lp.tauGy.lik(tau, y.j=y.j, sig.j=sig.j) + l.prior.logunif(tau))}
grid.tauGy.logunif <- apply(cbind(grid.tau), 1, p.tauGy.logunif) #unif prior on tau
norm.grid.logunif <- grid.tauGy.logunif/sum(grid.tauGy.logunif*width)
##ZOOM in on lower values of tau
plot(grid.tau, norm.grid.logunif, type="n", xlim=c(0,4), ylim=c(0,0.4), xlab="tau",
ylab="density")
lines(grid.tau, norm.grid.logunif, col=5, lwd=3, lty=1) #unif on log(tau)
lines(grid.tau, norm.tau.prior, col=2, lwd=2, lty=1) #folded-t prior
lines(grid.tau, norm.grid.tau, col=3, lwd=2)  #marg. post with foldedt
lines(grid.tau, norm.grid.tau2, col=4, lwd=3, lty=1) #unif on tau
legend(1.75,0.4, legend=c("folded-t prior (df=1, s=5)","p(tau|y) (p(tau) = folded-t)",
"p(tau|y) (p(tau)= unif)", "p(tau|y)  (p(log(tau))= unif)" ),lwd=c(3,3,3,3),
lty=c(1,1,1,1), col=c(2,3,4,5), bty="n")
###Could show that p(tau|y) for unif on log(tau) is not proper (See Exercise 5.10)
### Math: Everything in 5.21 (ignoring p(tau)) approaches a nonzero constant limit
### as tau goes to zero (see the p(tau|y) with p(tau) propto 1 in above plot)
### [i.e. the marginal likelihood p(y|tau) approaches finite non-zero value as tau->0]
###
### Therefore, the behavior of posterior near zero depends on the prior.
### Since p(log(tau)) = p(tau) propto 1/tau, and 1/tau is not integrable for any
### small interval including tau=0
### SO, take-home message is that if p(tau) is integrable near zero, then the
### posterior should be integrable near zero as well
### What about the other extreme?  What is happening as tau goes to infinity?
### The exponential term in 5.21 is less than or equal to 1, and with some algebra
### one can find the upper bound for p(tau|y) for tau large is
### p(tau)*sqrt(1/J)*(tau^(-(J-1))) where J is the number of groups
### Therefore, if p(tau) propto 1, it IS integrable as long as there are more than
### 2 groups (J>2), but Gelman et al. worry that with small J (4 or 5) it overestimates
### tau, resulting in less than optimal amount of pooling. For 1 or 2 groups is
### improper, but actually results in tau=infinity and no pooling.
### FOR MORE ABOUT PRIORS on tau, read section 5.7 and Gelman et al. (2006)
### Particularly pay attention to the commonly used priors
### Easy to think that if they work at lowest data level, they will work
### for variance params at next level, but that is not the case
### Page 129 quote: "In a hierarchical model the data can never rule out a
### group-level variance of zero, and so the prior distribution cannot put an
### infinite mass in this area".  Page 130 in BDA3 goes through different priors
### for the Schools example (TO BE ADDED IN FUTURE)
###### NOW, we will move away from just focusing on tau, to getting draws
###### from the joint posterior distribution p(theta,mu,tau|y)
###### To do this, we will use the Metropolis algorithm instead of grid
###### approximation to get draws of tau from p(tau|y)
##STEP 1 - draw tau from p(tau|y) for a particular p(tau)###
## We will sample from the two different p(tau|y)'s from  ##
## the folded-t and uniform priors together in same loop ##
nsamp <- 5000
tau.vec <- numeric(nsamp)  #save draws u
tau.vec2 <- numeric(nsamp) #save draws using uniform prior
tau.vec[1] <- 10  #starting value
tau.vec2[1] <- 10
for (t in 2:nsamp) {
tau.cur <- tau.vec[t-1] ##FOLDED-t sampling
#tau.cand <- r.tfold(1, df=2, mn=tau.cur) #jumping distn centered on current
tau.cand <- r.tfold(1, df=2, s=4, mn=0)
l.p.cand <- lp.tauGy.t1(tau.cand, y.j, sig.j)
l.p.cur <- lp.tauGy.t1(tau.cur, y.j, sig.j)
l.j.cand <- log(un.tfold(tau.cand, df=2, s=4))
l.j.cur <- log(un.tfold(tau.cur, df=2, s=4))
#l.j.cand <- log(d.tfold(tau.cand,df=2,ncp=tau.cur))  #not correct
#l.j.cur <- log(d.tfold(tau.cur,df=2,ncp=tau.cand))   #not correct
log.rat <- l.p.cand - l.p.cur + l.j.cur - l.j.cand
r <- min(1, exp(log.rat))
ifelse(runif(1)<r, tau.vec[t] <- tau.cand, tau.vec[t] <- tau.cur)
tau.cur2 <- tau.vec2[t-1]  #UNIFORM sampling
#tau.cand2 <- r.tfold(1, df=2, mn=tau.cur2) #jumping distn centered on curr
tau.cand2 <- r.tfold(1, df=2, s=4, mn=0)
l.p.cand2 <- lp.tauGy.unif(tau.cand2, y.j, sig.j)
l.p.cur2 <- lp.tauGy.unif(tau.cur2, y.j, sig.j)
l.j.cand2 <- log(un.tfold(tau.cand2, df=2, s=4))
l.j.cur2 <- log(un.tfold(tau.cur2, df=2, s=4))
#l.j.cand2 <- log(d.tfold(tau.cand2,df=2,ncp=tau.cur2)) #not correct
#l.j.cur2 <- log(d.tfold(tau.cur2,df=2,ncp=tau.cand2)) #not correct
log.rat2 <- l.p.cand2 - l.p.cur2 + l.j.cur2 - l.j.cand2
r2 <- min(1, exp(log.rat2))
ifelse(runif(1)<r2, tau.vec2[t] <- tau.cand2, tau.vec2[t] <- tau.cur2)
}
#Plot histograms of posterior draws of tau from the two priors
#Overlay curves obtained in first part of the code
par(mfrow=c(2,1))
hist(tau.vec, nclass=20, col=gray(.5), freq=F, main="Folded-t prior",
xlab="tau|y", xlim=c(0,50), ylim=c(0,0.2))
lines(grid.tau, norm.grid.tau, col=3, lwd=2)  #marg. post with foldedt
hist(tau.vec2, nclass=40, col=gray(.5), freq=F, main="Unif prior",
xlab="tau|y", xlim=c(0,50), ylim=c(0,0.2))
lines(grid.tau, norm.grid.tau2, col=4, lwd=3, lty=1) #unif on tau
##Look at sample path plots to see where the draws came from
dev.new()
plot(1:nsamp, tau.vec2, type="n", main="Sample path plots", ylim=c(0,60))
lines(1:nsamp, tau.vec2, col=3)
lines(1:nsamp, tau.vec, col=2) #looks more reasonable to me?
legend(100,58, legend=c("folded-t prior (df=1, s=5)","Uniform prior"),
lwd=c(2,2), lty=c(1,1), col=c(2,3), bty="n")
plot(1:nsamp, log(tau.vec2), type="n", main="Sample path plots", ylim=c(-5,5))
lines(1:nsamp, log(tau.vec2), col=3)
lines(1:nsamp, log(tau.vec), col=2) #looks more reasonable to me?
legend(100,5.2, legend=c("folded-t prior (df=1, s=5)","Uniform prior"),
lwd=c(2,2), lty=c(1,1), col=c(2,3), bty="n")
###STEP 2: Obtain samples from p(mu|tau,y) = Normal(mu.hat,V.mu)
#Function to easily get a draw from mu|tau,y
#conditional posterior of mu in Normal (Equation 5.20 in text)
r.mu.g.tau.y <- function(tau, y.j, sig.j) {
V.mu <- 1/(sum(1/(sig.j^2 + tau^2)))
mu.hat <- (sum((1/(sig.j^2 + tau^2))*y.j))*V.mu
draw <- rnorm(1, mu.hat, sd=sqrt(V.mu))
return(draw)
}
#r.mu.g.tau.y(1, y.j=y.j, sig.j=sig.j)
#Actually get the draws under the two different priors for tau
mu.vec <- apply(cbind(tau.vec), 1, r.mu.g.tau.y, y.j=y.j, sig.j=sig.j)
mu.vec2 <- apply(cbind(tau.vec2), 1, r.mu.g.tau.y, y.j=y.j, sig.j=sig.j)
#Take a look at results for mu|y under the two different priors
par(mfrow=c(2,1))
hist(mu.vec, nclass=50, col=gray(.7), freq=F, main="Folded-t prior",
xlab="mu|y", xlim=c(-10,30))
hist(mu.vec2, nclass=50, col=gray(.7), freq=F, main="Improper Unif. prior",
xlab="mu|y", xlim=c(-10,30))
#Look at summary measures to help compare the marginal distributions of mu|y
# under the two priors
summary(mu.vec)
summary(mu.vec2)
quantile(mu.vec, c(0.025, .975))
quantile(mu.vec2, c(0.025, .975))
mean(mu.vec>28) #0
mean(mu.vec2>28) #0.003
###STEP 3: Obtain samples from (theta_{j}|mu,tau,y)=N(theta.j.hat, Vj)
#Function to get a draw
r.thetaj.g.mu.tau.y <- function(mu.tau, y.j, sig.j) {
V.j <- 1/((1/(sig.j^2)) + (1/(mu.tau[2]^2)))
theta.hat.j <- ((y.j/(sig.j^2)) + (mu.tau[1]/(mu.tau[2]^2)))*V.j
theta.draws <- rnorm(8, theta.hat.j, sqrt(V.j))
return(theta.draws)
}
#Apply function to get vector of draws for each theta.j
theta.mat <- t(apply(cbind(mu.vec,tau.vec), 1, r.thetaj.g.mu.tau.y, y.j=y.j,
sig.j=sig.j))
theta.mat2 <- t(apply(cbind(mu.vec2,tau.vec2), 1, r.thetaj.g.mu.tau.y,
y.j=y.j, sig.j=sig.j))
###################################################################
### Now, let's look at school level results and further summarize #
burn.in <- 500  #Because based on Metropolis draws for tau
#Summarize the draws, make plots in text
school <- c("A","B","C","D","E","F","G","H")
dev.new()
par(mfrow=c(4,2))
for (j in 1:8) {
hist(theta.mat[-(1:burn.in),j], col=gray(.5), nclass=100, main=paste(school[j]),
xlim=c(-20,40), freq=FALSE)
}
dev.new()
par(mfrow=c(4,2))
for (j in 1:8) {
hist(theta.mat2[-(1:burn.in),j], col=gray(.5), nclass=100, main=paste(school[j]),
xlim=c(-20,40), freq=FALSE)
}
#### Replot so can directly compare the same school for different priors
dev.new()
par(mfrow=c(2,4)) #Compare results for schools 1 to 4
for (j in 1:4){
hist(theta.mat[-(1:burn.in),j], col=gray(.5), nclass=100, main=paste(school[j]),
xlim=c(-20,40), freq=FALSE, xlab=" ") } #Folded-t
for (j in 1:4){
hist(theta.mat2[-(1:burn.in),j], col=gray(.5), nclass=100, main=paste(school[j]),
xlim=c(-20,40), freq=FALSE, xlab=" ")} #Unif
dev.new()
par(mfrow=c(2,4))  #Compare results for schools 5 to 8
for (j in 5:8){
hist(theta.mat[-(1:burn.in),j], col=gray(.5), nclass=100, main=paste(school[j]),
xlim=c(-20,40), freq=FALSE, xlab=" ")} #Folded-t
for (j in 5:8){
hist(theta.mat2[-(1:burn.in),j], col=gray(.5), nclass=100, main=paste(school[j]),
xlim=c(-20,40), freq=FALSE, xlab=" ")} #Unif
### Find the posterior probability that
###   school A effect is as large as 28 points:
mean(theta.mat[,1]>28) #approx 0.01
mean(theta.mat2[,1]>28) #approx 0.05
## Figure 5.8 (b) The posterior dist'n for largest effect max(theta_{j})
## Note - these are NOT posterior predictive distributions
max.effect <- apply(theta.mat[-(1:burn.in),], 1, max)
max.effect2 <- apply(theta.mat2[-(1:burn.in),], 1, max)
dev.new()
par(mfrow=c(1,2))
hist(max.effect, col=gray(.5), nclass=100, main="max effect- folded-t", xlim=c(-20,70))
abline(v=28, col="orange", lwd=2, freq=FALSE)
hist(max.effect2, col=gray(.5), nclass=100, main="max effect- unif", xlim=c(-20,70))
abline(v=28, col="orange", lwd=2, freq=FALSE)
mean(max.effect > 28.4) #approx 0.016
mean(max.effect2 > 28.4) #approx 0.076
### Can we estimate the Pr(theta_A > theta_C|y)?  Posterior probability that school A is
##   more effective than school C?
mean(theta.mat[,1] > theta.mat[,3]) # approx 0.62
###############################################################
### First load data and run MCMC code to get posterior samples
### From code in SchoolsNormalHierarchicalExample_F15.R  #####
### theta.mat (folded-t) and theta.mat2 (unif) for tau   #####
### Updated Fall 2015                                    #####
##############################################################
### Think about residuals
### theta.mat is 5000 x 8
resid.fun <- function(theta.j, y.j){y.j - theta.j}
resid.out <- apply(theta.mat, 1, resid.fun, y.j=y.j) #8x5000
dev.new()
par(mfrow=c(3,3), mar=c(4,4,2,2))
for (s in 1:8){
plot(theta.mat[s,], resid.out[,s], pch=16, cex=1.2, ylim=c(-20,35), xlim=c(-15,30))
}
###Versus just plugging in the posterior mean of theta
theta.mns <- apply(theta.mat,2,mean)
plot(theta.mns, (y.j-theta.mns), pch=16, cex=1.2, ylim=c(-20,35), xlim=c(-15,30),
main="Using posterior MEAN", col=2)
### Think about average wt'd sum of squared resids (approx deviance)
avg.wt.sq.resid.fun <- function(theta.j, y.j, sig.j) {
mean(((y.j-theta.j)^2)/(sig.j^2))
}
avg.sq.resid.pm <- avg.wt.sq.resid.fun(theta.mns, y.j=y.j, sig.j=sig.j) #at posterior means
avg.sq.resid.pool <- avg.wt.sq.resid.fun(rep(mean(y.j),8), y.j=y.j, sig.j=sig.j) #at overall mean
##Do for EACH posterior draw to get posterior distribution
post.avg.sq.resid <- apply(theta.mat, 1, avg.wt.sq.resid.fun, y.j=y.j, sig.j=sig.j)
par(mfrow=c(1,1))
hist(post.avg.sq.resid, nclass=100, col=gray(0.9), main="Posterior of Average Weighted Squared Resids",
freq=FALSE)
abline(v=avg.sq.resid.pm, lwd=2, col=2)  #Using posterior mean as plug-in
abline(v=avg.sq.resid.pool, lwd=2, col=4)  #using overall mean as plug-in
legend(1.5,3.0, legend=c("posterior means", "overall mean"), col=c(2,4),
lwd=c(2,2), lty=c(1,1), bty="n")
## CHANGED IN _F15 version
## Now each y.rep is only used with its generating theta, rather than moving
## over all theta for a y.rep like we do for the observed data.
## BUT, can also do for y.rep's (for these 8 schools)
n_draws <- length(theta.mat[,1])
y.rep.mat <- matrix(NA, nrow=n_draws, ncol=8)
for (j in 1:8){
y.rep.mat[,j] <- rnorm(n_draws, mean=theta.mat[,j], sd=sig.j[j])
}
#Write function to calculate wt'd MSE for different draws of (theta, yrep)
T.yrep.mse.fun <- function(theta_yrep, sig.j) {
avg.wt.sq.resid.fun(theta.j=theta_yrep[1:8], y.j=theta_yrep[9:16], sig.j=sig.j)
}
theta_yrep_mat <- cbind(theta.mat,y.rep.mat) #5000 x 16
choose.draws <- sample(500:5000, size=1000) #choose draws randomly after 500
T.yrep.theta <- apply(theta_yrep_mat[choose.draws,], 1, T.yrep.mse.fun, sig.j=sig.j)
par(mfrow=c(1,2))
plot(post.avg.sq.resid[choose.draws], T.yrep.theta, pch=16, cex=1.2, xlim=c(0,5),
ylim=c(0,5), xlab="Posterior MSE (y_obs)", ylab="Posterior MSE (y_preds)")
abline(a=0, b=1)
abline(v=c(avg.sq.resid.pm,avg.sq.resid.pool), col="orange")
#Why would it make sense for ypreds to have more larger values for this application?
#Proportion of posterior predictive draws such that the T(yrep,theta) is greater
# than or equal to T(yobs)?
mean(T.yrep.theta >= post.avg.sq.resid[choose.draws]) #.707
### Or histograms of differences?
dev.new()
hist((post.avg.sq.resid[choose.draws]-T.yrep.theta), col=gray(0.8), nclass=100, xlim=c(-3,3),
main="T(y,theta)-T(yrep,theta)", xlab="Difference")
abline(v=0, col=2)
avg.sq.resid.pm <- avg.wt.sq.resid.fun(theta.mns, y.j=y.j, sig.j=sig.j) #at posterior means
avg.sq.resid.pool
### More general: Compare deviance among models:
Dev.fun <- function(theta.j, y.j, sig.j) {
-2*sum(log(dnorm(y.j, theta.j, sig.j)))
}
Dev.fun(y.j, y.j, sig.j)
#1. Calculate D.theta.hat
burn.in <- 500
#a. Find a theta.hat - use posterior means
post.mns <- apply(theta.mat[-(1:burn.in),],2,mean)
post.mns2 <- apply(theta.mat2[-(1:burn.in),],2,mean)
#b. Plug in posterior means to deviance function
D.theta.hat <- Dev.fun(post.mns, y.j, sig.j)     #58.35
D.theta.hat2 <- Dev.fun(post.mns2, y.j, sig.j)   #57.40 ("matches" book)
#2. Calculate D.avg.hat
#a. Apply the deviance function to every set of posterior draws of theta
Dev.draws <- apply(theta.mat[-(1:burn.in),],1,Dev.fun, y.j=y.j, sig.j=sig.j)
Dev.draws2 <- apply(theta.mat2[-(1:burn.in),],1,Dev.fun, y.j=y.j, sig.j=sig.j)
par(mfrow=c(2,1))
hist(Dev.draws, col=gray(.5), nclass=100, main="Deviance (folded-t)", xlim=c(50,80),
xlab=expression(D(y,theta)), freq=F, ylim=c(0,0.4))
abline(v=D.theta.hat, col=2, lwd=2)
text(56,0.4, expression(D[hat(theta)]))
hist(Dev.draws2, col=gray(.5), nclass=100, main="Deviance (uniform)", xlim=c(50,80),
xlab=expression(D(y,theta)),freq=F, ylim=c(0,0.4))
abline(v=D.theta.hat2, col=2, lwd=2)
text(55,0.4, expression(D[hat(theta)]))
#b. Average the deviance over all the draws
D.avg.hat <- mean(Dev.draws)     #60.227
D.avg.hat2 <- mean(Dev.draws2)   #60.2916
abline(v=D.avg.hat2, col="orange", lwd=2) #add to histogram for uniform
text(65, 0.4, expression(bar[D(theta)]))
#3. Calculate the effective number of parameters (pD) using the two
#    different methods in the book (pg 181,182): pD1 and pD2
pD1 <- D.avg.hat - D.theta.hat       #1.9
pD1.2 <- D.avg.hat2 - D.theta.hat2   #2.78
#pD2 is approximately 1/2 the posterior variance of the deviance
pD2 <- 0.5*var(Dev.draws)  #1.82
pD2.2 <- 0.5*var(Dev.draws2) #2.48
#4. Combine to get DIC= D.pred.avg.hat = 2*D.avg.hat - D.theta.hat
#                                      = D.avg.hat + p.D1
DIC.foldedt <- D.avg.hat + pD1    #62.03
DIC.uniform <- D.avg.hat2 + pD1.2   #63.03
#5. Get DIC for the no pooling (tau=infinity) model (separate means model)
D.theta.hat.nopool <- Dev.fun(y.j, y.j, sig.j)  #54.64
#Drawing nsamp-burn.in draws of 8 draws from separate means and variances
theta.mat.nopool <- t(replicate(nsamp-burn.in, rnorm(8, mean=y.j, sd=sig.j))) #4500x8
Dev.draws.nopool <- apply(theta.mat.nopool,1, Dev.fun, y.j=y.j, sig.j=sig.j)
D.avg.hat.nopool <- mean(Dev.draws.nopool)  #62.65
pD1.nopool <- D.avg.hat.nopool - D.theta.hat.nopool  #8.00
DIC.nopool <- D.avg.hat.nopool + pD1.nopool  #70.66
#6. Get DIC for the complete pooling (tau=0) model (single mean model)
mu.hat.tau0 <- sum(y.j/(sig.j^2))/sum(1/(sig.j^2)) #See page 136
sig.mu.tau0 <- sqrt(1/(sum(1/(sig.j^2))))
D.theta.hat.pool <- Dev.fun(rep(mean(mu.vec[burn.in:5000]),8), y.j, sig.j) #59.41682
theta.mat.pool <- t(replicate(nsamp-burn.in, rnorm(8, mean=mu.hat.tau0, sd=sig.mu.tau0)))
Dev.draws.pool <- apply(theta.mat.pool,1, Dev.fun, y.j=y.j, sig.j=sig.j)
D.avg.hat.pool <- mean(Dev.draws.pool)  #60.3
pD1.pool <- D.avg.hat.pool - D.theta.hat.pool  #0.99
DIC.pool <- D.avg.hat.pool + pD1.pool   #61.3
####Compare posterior deviance distributions between hiearch. and
par(mfrow=c(2,2))
hist(Dev.draws, col=gray(.5), nclass=100, main="Deviance (folded-t)", xlim=c(50,80),
xlab=expression(D(y,theta)), freq=F, ylim=c(0,0.4))
abline(v=c(D.theta.hat, D.avg.hat, DIC.foldedt), col=c(2,4,5), lwd=2)
text(56,0.4, expression(D[hat(theta)]))
hist(Dev.draws2, col=gray(.5), nclass=100, main="Deviance (uniform)", xlim=c(50,80),
xlab=expression(D(y,theta)),freq=F, ylim=c(0,0.4))
abline(v=c(D.theta.hat2, D.avg.hat2, DIC.uniform), col=c(2,4,5), lwd=2)
text(55,0.4, expression(D[hat(theta)]))
hist(Dev.draws.nopool, col=gray(.5), nclass=100, main="Deviance (No pooling )", xlim=c(50,80),
xlab=expression(D(y,theta)), freq=F, ylim=c(0,0.4))
abline(v=c(D.theta.hat.nopool, D.avg.hat.nopool, DIC.nopool), col=c(2,4,5), lwd=2)
text(56,0.4, expression(D[hat(theta)]))
text(65,0.4, expression(bar(D(theta))))
text(72,0.4, "DIC")
hist(Dev.draws.pool, col=gray(.5), nclass=100, main="Deviance (Complete Pool)", xlim=c(50,80),
xlab=expression(D(y,theta)),freq=F, ylim=c(0,0.4))
abline(v=c(D.theta.hat.pool, D.avg.hat.pool, DIC.pool), col=c(2,4,5), lwd=2)
text(55,0.4, expression(D[hat(theta)]))
#### Make table of results like Table 6.2
out.table <- data.frame(
D.theta.hat=round(c(D.theta.hat,D.theta.hat2,D.theta.hat.nopool,D.theta.hat.pool), digits=2),
D.avg.hat= round(c(D.avg.hat,D.avg.hat2,D.avg.hat.nopool,D.avg.hat.pool), digits=2),
pD=round(c(pD1, pD1.2, pD1.nopool, pD1.pool),digits=2),
DIC=round(c(DIC.foldedt, DIC.uniform, DIC.nopool, DIC.pool), digits=2),
row.names=c("Folded-t", "Uniform", "No pooling", "Complete pool")
)
out.table
head(theta.mat)
#### Make table of results like Table 6.2
out.table <- data.frame(
D.theta.hat=round(c(D.theta.hat,D.theta.hat2,D.theta.hat.nopool,D.theta.hat.pool), digits=2),
D.avg.hat= round(c(D.avg.hat,D.avg.hat2,D.avg.hat.nopool,D.avg.hat.pool), digits=2),
pD=round(c(pD1, pD1.2, pD1.nopool, pD1.pool),digits=2),
DIC=round(c(DIC.foldedt, DIC.uniform, DIC.nopool, DIC.pool), digits=2),
row.names=c("Folded-t", "Uniform", "No pooling", "Complete pool")
)
out.table
sum.sq.resid.yrep.fun <- function(y.rep, E.yrep, sig.j) {
#mean(((y.rep-E.yrep)^2))
sum(((y.rep-E.yrep)/sig.j)^2)
}
yrep.post.mns <- apply(y.rep.mat, 2, mean)
T2.yrep.theta <- apply(y.rep.mat, 1, sum.sq.resid.yrep.fun, E.yrep=yrep.post.mns, sig.j=sig.j)
T2.yrep.DICcompare <- -2*(sum(-0.5*(log(2*pi*(sig.j^2)))) - 0.5*T2.yrep.theta)
mean(T2.yrep.DICcompare)  #64.32
Dev.pred.fun <- function(yrep,theta.hat, sig.j) {
-2*sum(dnorm(yrep, theta.hat, sig.j, log=TRUE))
}
D.pred <- apply(y.rep.mat,1,Dev.pred.fun, theta.hat=post.mns, sig.j=sig.j)
dev.new()
hist(D.pred, col=gray(.5), nclass=100, main="Post Pred. Deviance at Posterior Means",
xlab="Post Pred Deviance", freq=F, xlim=c(40,150))
abline(v=DIC.foldedt, col=c(5,"purple","purple"), lwd=2)
D.pred.avg <- mean(D.pred) #64.32
par(mfrow=c(2,1))
hist(Dev.draws, col=gray(.5), nclass=100, main="Deviance (folded-t)", xlim=c(50,80),
xlab=expression(D(y,theta)), freq=F, ylim=c(0,0.4))
abline(v=c(D.theta.hat, D.avg.hat, DIC.foldedt), col=c(2,4,5), lwd=2)
text(56,0.4, expression(D[hat(theta)]))
hist(T2.yrep.DICcompare, col=gray(.5), nclass=150, main="Post. Pred. MSE (DIC scale)",
xlab="post pred MSE (Dev. scale)", freq=F, xlim=c(50,80))
abline(v=c(DIC.foldedt, mean(T2.yrep.DICcompare), median(T2.yrep.DICcompare)),
col=c(5,"purple","magenta"), lwd=2)
abline(v=D.pred.avg, col="orange", lwd=2)
SSPE.fun <- function(y.rep, y, sig.j) {
#mean(((y-y.rep)^2))
sum(((y-y.rep)/sig.j)^2)
}
T.SSPE <- apply(y.rep.mat, 1, SSPE.fun, y=y.j, sig.j=sig.j)
T.SSPE.DevScale <- -2*(sum(-0.5*(log(2*3.1415926*(sig.j^2)))) - 0.5*T.SSPE)
mean(T.SSPE.DevScale) #68.17
l.p.y.g.theta.sepj.fun <- function(theta.j, y.j, sig.j) {
dnorm(y.j, theta.j, sig.j, log=TRUE)
}
#l.p.y.g.theta.sepj.fun(theta.mat[2,], y.j, sig.j)
p.y.g.theta.sepj.fun <- function(theta.j, y.j, sig.j) {
dnorm(y.j, theta.j, sig.j)
}
#p.y.g.theta.sepj.fun(theta.mat[2,], y.j, sig.j)
lp.y.g.theta.sepj <- apply(theta.mat, 1, l.p.y.g.theta.sepj.fun, y.j=y.j, sig.j=sig.j) #2nd part of pWAIC
dim(lp.y.g.theta.sepj) #8 x 5000
### Calculate p.y.g.theta for each theta, then average, then take log to get computed lppd?
p.y.g.theta.sepj <- apply(theta.mat, 1, p.y.g.theta.sepj.fun, y.j=y.j, sig.j=sig.j) #calculate for each theta
mean.dev.sepj <- apply(p.y.g.theta.sepj, 1, mean) #average over theta
log.mean.dev.sepj <- log(mean.dev.sepj) #take log
est.lppd <- sum(log.mean.dev.sepj)
#### Calculate the pWAIC1
pD.WAIC1 <- 2*(sum(log.mean.dev.sepj - mean(lp.y.g.theta.sepj)))
pD.WAIC1
#### Calculate the pWAIC2
var.sepj <- apply(lp.y.g.theta.sepj, 1, var)
pD.WAIC2 <- sum(var.sepj)  #See Equation 7.12 in text #1.009749
pD.WAIC2
### Calculate WAIC2 on deviance scale
WAIC2 <- -2*(est.lppd - pD.WAIC2)  #61.40
WAIC2
WAIC1 <- -2*(est.lppd - pD.WAIC1)
WAIC1
### For the Uniform prior  ###
lp.y.g.theta.sepj.2 <- apply(theta.mat2, 1, l.p.y.g.theta.sepj.fun, y.j=y.j, sig.j=sig.j) #2nd part of pWAIC
p.y.g.theta.sepj.2 <- apply(theta.mat2, 1, p.y.g.theta.sepj.fun, y.j=y.j, sig.j=sig.j) #calculate for each theta
mean.dev.sepj.2 <- apply(p.y.g.theta.sepj.2, 1, mean) #average over theta
log.mean.dev.sepj.2 <- log(mean.dev.sepj.2) #take log
est.lppd.2 <- sum(log.mean.dev.sepj.2)
var.sepj.2 <- apply(lp.y.g.theta.sepj.2, 1, var)
pD.WAIC2.2 <- sum(var.sepj.2)  #See Equation 7.12 in text #1.009749
WAIC2.2 <- -2*(est.lppd.2 - pD.WAIC2.2)
#### Look at approximate classical estimate of tau2 - using Deviance from one mean model (red) vs.
####  SS of 8 mean model (full)   tau2.hat <- MSB - MSW
ExtraDev <- (59.35-54.64)
ExtraDf <- (8-1)
FullDev <- 54.54
FullDf <- 8
tau.2.est <- (ExtraDev/ExtraDf) - (FullDev/FullDf)
library(xtable)
xtable(out.table)
tau.2.est
#### Conditional AIC -
#### How to count parameters for the hierachical model?
### What if used hierarchical model, but only interested in the overall mean and not the means of the
####  individual schools?
#Calculate AIC:   AIC = -2log(p(y|mu.hat,tau.hat)) + 2p
AIC.hm.uniform <- D.theta.hat2 + 2*2     #61.548 = 2 params are mu and tau2?
AIC.hm.foldedt <- D.theta.hat + 2*2      #62.404 = 2 params are mu and tau2
AIC.pool <- D.theta.hat.pool + 2*1       #61.349
AIC.nopool <- D.theta.hat.nopool + 2*8   #70.641
out.table <- data.frame(
D.theta.hat=round(c(D.theta.hat,D.theta.hat2,D.theta.hat.nopool,D.theta.hat.pool), digits=2),
D.avg.hat= round(c(D.avg.hat,D.avg.hat2,D.avg.hat.nopool,D.avg.hat.pool), digits=2),
pD=round(c(pD1, pD1.2, pD1.nopool, pD1.pool),digits=2),
DIC=round(c(DIC.foldedt, DIC.uniform, DIC.nopool, DIC.pool), digits=2),
AIC=round(c(AIC.hm.foldedt, AIC.hm.uniform, AIC.nopool, AIC.pool), digits=2),
WAIC=round(c(WAIC2, WAIC2.2, NA, NA), digits=2),
row.names=c("Folded-t", "Uniform", "No pooling", "Complete pool")
)
out.table
