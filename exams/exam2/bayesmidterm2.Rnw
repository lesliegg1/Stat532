\documentclass[12pt]{article}

\usepackage{amssymb,amsmath}
\usepackage{enumerate}
\usepackage{verbatim}
\usepackage{graphicx, multicol}
\usepackage{float}

%% LaTeX margin settings:
  \setlength{\textwidth}{7.0in}
\setlength{\textheight}{9in}
\setlength{\oddsidemargin}{-.5in}
\setlength{\evensidemargin}{0in}
\setlength{\topmargin}{-1.5cm}

%% tell knitr to use smaller font for code chunks
\def\fs{\footnotesize}
\def\R{{\sf R}}
\newcommand{\bfbeta}{\mbox{\boldmath $\beta$}}
\newcommand{\bfD}{\mbox{\boldmath $D$}}
\newcommand{\bfL}{\mbox{\boldmath $L$}}
\newcommand{\bfR}{\mbox{\boldmath $R$}}
\newcommand{\bfmu}{\mbox{\boldmath $\mu$}}
\newcommand{\bfv}{\mbox{\boldmath $V$}}
\newcommand{\bfX}{\mbox{\boldmath $X$}}
\newcommand{\bfy}{\mbox{\boldmath $y$}}
\newcommand{\bfb}{\mbox{\boldmath $b$}}
\begin{document}


<<setup, include=FALSE, cache=FALSE>>=
  opts_chunk$set(fig.width=5, fig.height=4, out.width='.5\\linewidth', dev='pdf', concordance=TRUE, size='footnotesize')
options(replace.assign=TRUE,width=72, digits = 3, 
        show.signif.stars = FALSE)
@
  
  
\begin{center}
\large{Bayes Midterm 2} \\
Leslie Gains-Germain
\end{center}

\begin{enumerate}

\item \begin{enumerate}

\item The following example was inspired by Claire and Jordan's consulting project. Suppose there are five reservations in Montana. The number of children eligible to receive dental sealants on the reservation is unknown because it's hard to keep track of this population. A success is observed when a child comes to a dental clinic to have dental sealants applied. The number of children on each of the five reservations who had dental sealants applied in $2014$ was observed. The probability of having the sealants applied and the number of eligible children is assumed to be the same for all five reservations. 

\item The MLE occurs near $m=101, \pi=0.21$. 

<<likelihood, echo=FALSE, out.width="\\linewidth", fig.width=8>>=
loglik.fun <- function(m.pi, y.vec){
  sum(lchoose(m.pi[1], y.vec)+y.vec*log(m.pi[2])+(m.pi[1]-y.vec)*log(1-m.pi[2]))
}

y1.data <- c(16, 18, 22, 25, 27) 

m <- seq(27, 250, by=1)
pi <- seq(0.05, 0.9, length=length(m))
grid.vals <- expand.grid(m=m, pi=pi)
grid.vals <- as.matrix(grid.vals)
loglik.vals <- apply(grid.vals, 1, loglik.fun, y.vec=y1.data)

lik.vals <- exp(loglik.vals-max(loglik.vals))
lik.mat <- matrix(lik.vals, nrow=length(m), ncol=length(pi))

#find where max occurs on grid
grid.max <- grid.vals[which(loglik.vals==max(loglik.vals)),]

contour(m, pi, lik.mat, levels=seq(0.05, 0.95, 0.25), xlim=c(20, 230), 
        ylim=c(0, 0.8), 
        xlab=expression(m), ylab=expression(pi), 
        main=expression(paste(y[1], " data")))
points(grid.max[1], grid.max[2], col="red", pch=16, cex=2)
@

\item The MLE occurs near $m=196, \pi=0.11$. 

<<likelihood2, echo=FALSE, out.width="\\linewidth", fig.width=8>>=
loglik.fun <- function(m.pi, y.vec){
  sum(lchoose(m.pi[1], y.vec)+y.vec*log(m.pi[2])+(m.pi[1]-y.vec)*log(1-m.pi[2]))
}

y2.data <- c(16, 18, 22, 25, 28) 

m <- seq(28, 250, by=1)
pi <- seq(0.05, 0.9, length=length(m))
grid.vals <- expand.grid(m=m, pi=pi)
grid.vals <- as.matrix(grid.vals)
loglik.vals <- apply(grid.vals, 1, loglik.fun, y.vec=y2.data)

lik.vals <- exp(loglik.vals-max(loglik.vals))
lik.mat <- matrix(lik.vals, nrow=length(m), ncol=length(pi))

#find where max occurs on grid
grid.max <- grid.vals[which(loglik.vals==max(loglik.vals)),]

contour(m, pi, lik.mat, levels=seq(0.05, 0.95, 0.25), xlim=c(20, 230), 
        ylim=c(0, 0.8), 
        xlab=expression(m), ylab=expression(pi), 
        main=expression(paste(y[2], " data")))
points(grid.max[1], grid.max[2], col="red", pch=16, cex=2)
@

\item \begin{enumerate}

\item The joint posterior distribution of $(m, \pi)$ is shown below.

<<jointpost, echo=FALSE, out.width="\\linewidth", fig.width=6, warning=FALSE>>=
log.prior.fun <- function(m.pi) {
  -lfactorial(m.pi[1])+m.pi[1]*log(100)-100
}

log.post.fun <- function(m.pi, y.vec) {
  loglik.fun(m.pi, y.vec) + log.prior.fun(m.pi)
}

log.post.vals <- apply(grid.vals, 1, log.post.fun, y.vec=y2.data)

post.vals <- exp(log.post.vals-max(log.post.vals))
post.mat <- matrix(post.vals, nrow=length(m), ncol=length(pi))

contour(m, pi, lik.mat, levels=seq(0.05, 0.95, 0.25), xlim=c(20, 230), ylim=c(0, 0.8), 
        xlab=expression(m), ylab=expression(pi), main="Posterior and Likelihood", 
        lwd=3, col="red")

contour(m, pi, post.mat, levels=seq(0.05, 0.95, 0.25), lwd=3, add=TRUE, col="black")
points(grid.max[1], grid.max[2], col="blue", pch=16, cex=2)

legend(150, 0.6, bty="n", legend=c("Likelihood", "Posterior"), lwd=c(3,3), 
       col=c("red", "black"), cex=1.1)
@

\item My work for deriving the complete conditionals for $m$ and $\pi$ is shown below.
\begin{align*}
p(\pi|y, m) &= \frac{p(\pi, y, m)}{p(y, m)} \\
&\propto p(\pi, y, m) \\
&\propto p(y|\pi, m)p(m, \pi) \\
&\propto p(y|\pi, m)p(\pi) \\
p(m|y, \pi) &= \frac{p(\pi, y, m)}{p(y, \pi)} \\
&\propto p(\pi, y, m) \\
&\propto p(y|\pi, m)p(m, \pi) \\
&\propto p(y|\pi, m)p(m)
\end{align*}

\item My code for the Gibbs sampler is shown below.

<<gibbs, echo=TRUE>>=
#function to calculate complete conditional for pi on log scale
log.pi.cc.fun <- function(m.pi, y.vec){
  loglik.fun(m.pi, y.vec)
}

#check function
#log.pi.cc.fun(c(40, 0.5), y2.data)

#function to calculate complete conditional for m on log scale
log.m.cc.fun <- function(m.pi, y.vec){
  log.post.fun(m.pi, y.vec)
}

#check function
#log.m.cc.fun(c(40, 0.5), y2.data)

nchain <- 3
nsim <- 10000
m.pi.mat <- array(NA, dim=c(nsim, 2, nchain))

#keep track of acceptance ratios
jump.mat <- matrix(NA, nrow=nsim-1, ncol=2)

#specify starting values for each chain
m.pi.mat[1, 1:2, 1] <- c(100, 0.4)
m.pi.mat[1, 1:2, 2] <- c(50, 0.2)
m.pi.mat[1, 1:2, 3] <- c(150, 0.4)

#define standard deviations for normal proposal distributions
sd.scale <- c(10, 5)

set.seed(230923)

for (j in 1:nchain) {
  for (i in 2:nsim) {
    #set pi equal to the starting value
    pi <- m.pi.mat[i-1, 2, j]
    
    #now draw m from complete conditional with Metropolis Hastings Algorithm
    m.cur <- m.pi.mat[i-1, 1, j]
    m.cand <- rpois(1, lambda=m.cur)
    
    log.r.num.m <- log.m.cc.fun(c(m.cand, pi), y.vec = y2.data) + 
                     dpois(m.cur, lambda=m.cand, log = TRUE)
    
    log.r.denom.m <- log.m.cc.fun(c(m.cur, pi), y.vec = y2.data) + 
                       dpois(m.cand, lambda=m.cur, log = TRUE)
    
    log.r.m <- log.r.num.m - log.r.denom.m
    
    p.accept.m <- min(1, exp(log.r.m))
    
    u.vec <- runif(2)
    
    ifelse(u.vec[1] <= p.accept.m, m.pi.mat[i, 1, j] <- m.cand,
           m.pi.mat[i, 1, j] <- m.cur)
    
    jump.mat[i-1, 1] <- ifelse(u.vec[1] <= p.accept.m, 1, 0)
    
    #now fix m at its value in the ith iteration
    m <- m.pi.mat[i, 1, j]
    
    #draw pi from complete conditional with metropolis hastings algorithm
    pi.cur <- m.pi.mat[i-1, 2, j]
    pi.cand <- rbeta(1, 2, sd.scale[2])
    
    log.r.num.pi <- log.pi.cc.fun(c(m, pi.cand), y.vec = y2.data) + 
                     dbeta(pi.cur, 2, sd.scale[2], log = TRUE)
    
    log.r.denom.pi <- log.pi.cc.fun(c(m, pi.cur), y.vec = y2.data) + 
                       dbeta(pi.cand, 2, sd.scale[2], log = TRUE)
    
    log.r.pi <- log.r.num.pi - log.r.denom.pi
    
    p.accept.pi <- min(1, exp(log.r.pi))
    
    u.vec <- runif(2)
    
    ifelse(u.vec[2] <= p.accept.pi, m.pi.mat[i, 2, j] <- pi.cand,
           m.pi.mat[i, 2, j] <- pi.cur)
    
    jump.mat[i-1, 2] <- ifelse(u.vec[2] <= p.accept.pi, 1, 0)
  }
}

@

<<traceplots, echo=FALSE, include=FALSE, message=FALSE>>=
plot(seq(1:nsim), m.pi.mat[1:nsim, 1, 1], type="l", ylab=expression(m))
   lines(seq(1:nsim), m.pi.mat[1:nsim, 1, 2], col=2)
   lines(seq(1:nsim), m.pi.mat[1:nsim, 1, 3], col=3)
   
plot(seq(1:nsim), m.pi.mat[1:nsim, 2, 1], type="l", ylab=expression(pi))
   lines(seq(1:nsim), m.pi.mat[1:nsim, 2, 2], col=2)
   lines(seq(1:nsim), m.pi.mat[1:nsim, 2, 3], col=3)

burnin <- 1000   
   
require(coda)
m.post1 <- mcmc(m.pi.mat[burnin:nsim,1,1])
m.post2 <- mcmc(m.pi.mat[burnin:nsim,1,2])
m.post3 <- mcmc(m.pi.mat[burnin:nsim,1,3])
m.mcmc <- mcmc.list(list(m.post1, m.post2, m.post3))
eff <- effectiveSize(m.mcmc)
gelman.diag(m.mcmc)

pi.post1 <- mcmc(m.pi.mat[burnin:nsim,2,1])
pi.post2 <- mcmc(m.pi.mat[burnin:nsim,2,2])
pi.post3 <- mcmc(m.pi.mat[burnin:nsim,2,3])
pi.mcmc <- mcmc.list(list(pi.post1, pi.post2, pi.post3))
eff <- effectiveSize(pi.mcmc)
gelman.diag(pi.mcmc)
@

\end{enumerate}

\item The marginal posterior distributions for $m$ and $\pi$ are shown below. The posterior probability that $m>100$ is $0.443$ and the posterior probability that $\pi < 0.3$ is $0.989$. Since $m$ and $\pi$ are independent, the joint posterior probability that $m > 100$ and $\pi < 0.3$ is $0.443*0.989 = 0.438$.

<<converdiags, echo=FALSE, out.width="\\linewidth", fig.width=8>>=
par(mfrow=c(1,2))

#plot draws
m.draws <- as.matrix(m.pi.mat[burnin:nsim, 1, ])
hist(m.draws, freq=FALSE, nclass=50, xlab=expression(m))

pi.draws <- as.matrix(m.pi.mat[burnin:nsim, 2, ])
hist(pi.draws, freq=FALSE, nclass=50, xlab=expression(pi))

#mean(m.draws>100)
#mean(pi.draws<0.3)
@

\item disads

\item The hierarchical version of the model puts priors on $\lambda$ and priors on parameters of a $Beta(\alpha, \beta)$ distribution. I chose to put priors on the prior mean, $\eta = \frac{\alpha}{\alpha+\beta}$ and the somewhat approximate prior standard deviation, $\sigma = \sqrt{\frac{1}{\alpha + \beta}}$, kind of like what we did in the Beta-Binomial example with the stomach cancer data.
\begin{align*}
y_i &\sim Bin(m, \pi) \\
\pi &\sim Beta(\frac{\eta}{\sigma^2}, \frac{1-\eta}{\sigma^2}) \\
\sigma &\sim Uniform(0, 100) \\
\eta &\sim Uniform(0, 1) \\
\lambda &\sim Uniform(0, 500)
\end{align*}

\item The JAGs code I used to obtain draws from the above model is shown below.

<<jagsmodelbetabinom, echo=TRUE>>=
##write model file first
cat("
model
{
for(i in 1:N)
{
y[i] ~ dbin(pi, m)
}

pi ~ dbeta(eta/sigma^2, (1-eta)/sigma^2)
eta ~ dunif(0, 1)
sigma ~ dunif(0, 100)

m ~ dpois(lambda)
lambda ~ dunif(0, 500)
}",
file="model1.jags")
@


<<jagscallbetabin, echo=TRUE, results='hide', message=FALSE, cache=TRUE>>=
##jags call
library(R2jags)
set.seed(52)


dental.data <- list(N=length(y2.data), y=y2.data)

inits <- list(list(pi=0.4, eta = .00003, m = 100, lambda = 500, 
                   sigma = 1.20), 
              list(pi=0.2, eta = .00003, m = 50, lambda = 150, 
                   sigma = 2.24),
              list(pi=0.4, eta = .00003, m = 150, lambda = 300, 
                   sigma = 1.58))
n.chain <- 3

#warmup
warmup.model1 <- jags.model("model1.jags", data=dental.data, n.chains=n.chain, inits= inits, n.adapt=4000, quiet=TRUE)

#parameters to save
params <- c("pi", "eta", "sigma", "m", "lambda")

n.iter=50000
#running the model for real
model1 <- coda.samples(warmup.model1, params, n.iter=n.iter)
@

\item The plots below compare the posterior distributions for $\pi$ and $m$ obtained in part (f) to those from the hierarchical model. There is much more variability in the posterior draws for $m$ and $\pi$ in the hierarchical model compared to the model with fixed hyperparameters. I think this makes sense because when the hyperparameter values are approximated, the model does not incorporate uncertainty in these parameters. With such a small sample size, the additional source of uncertainty accounted for in the hierarchical model makes a big difference in the results.

<<jagsdrawscompare, echo=FALSE, out.width="\\linewidth", fig.width=10>>=
#par(mfrow=c(2,3))
#traceplot(model1)

m.jags.draws <- as.matrix(model1)[,3]
pi.jags.draws <- as.matrix(model1)[,4]

par(mfrow=c(1,2))

hist(m.jags.draws, freq=FALSE, nclass=40, xlab=expression(m), ylim=c(0, 0.04), main="m", col="lightgreen")
hist(m.draws, freq=FALSE, nclass=50, add=TRUE)
legend(120, 0.03, bty="n", c("Fixed hyperparams", "Hierarchical"), fill=c("white", "lightgreen"), cex=0.8)

hist(pi.jags.draws, freq=FALSE, nclass=30, xlab=expression(pi), main=expression(pi), ylim=c(0, 16), col="lightgreen")
hist(pi.draws, freq=FALSE, nclass=50, add=TRUE)
legend(0.25, 10, bty="n", c("Fixed hyperparams", "Hierarchical"), fill=c("white", "lightgreen"), cex=0.8)
@

The posterior distributions of the hyperparameters are shown below, with the fixed values used in (e).

<<jagshyperdraws, echo=FALSE, out.width="\\linewidth", fig.width=10>>=
par(mfrow=c(1,3))
lambda.draws <- as.matrix(model1)[,2]
sigma.draws <- as.matrix(model1)[,5]
eta.draws <- as.matrix(model1)[,1]

hist(lambda.draws, main=expression(lambda), freq=FALSE, xlab=expression(lambda))
abline(v=100, lwd=3, col="red")

hist(sigma.draws, main=expression(sigma), freq=FALSE, xlab=expression(sigma))
abline(v=0.707, lwd=3, col="red")

hist(eta.draws, main=expression(eta), freq=FALSE, xlab=expression(eta))
abline(v=0.5, lwd=3, col="red")
@

\item

\end{enumerate}

<<readdata, echo=FALSE>>=
setwd("~/Documents/Stat532/exams/exam2")
frog.data <- read.csv("FrogDoseData.csv")
@

\item \begin{enumerate}

\item The simple logistic regression model for estimating the dose-death relationship is as follows. I treat dose as a continuous predictor variable. The empirical probabilities of death at each dose as well as the fitted probabilities from the glm() model are shown on the plot below. The dose at which half of the frogs are estimated to die is $\frac{0.5 + 56.97}{78.29} = 0.734$ grams per ml. 
\begin{align*}
y_{ij} \sim& Bin(5, \pi_j) \\
logit(\pi_j) &= \alpha + \beta x_j
\end{align*}

\begin{center}
<<glmmodel, echo=FALSE, message=FALSE>>=
glm.fit <- glm(death ~ dose, data = frog.data, family = "binomial")

require(arm)

dose1 <- with(frog.data, sum(death[dose==0.42])/5)
dose2 <- with(frog.data, sum(death[dose==0.71])/5)
dose3 <- with(frog.data, sum(death[dose==0.98])/5)
dose4 <- with(frog.data, sum(death[dose==2.13])/5)

emp.probs <-  c(dose1, dose2, dose3, dose4)
fitted.probs <- fitted(glm.fit)

plot(frog.data$dose, fitted.probs, type="n", ylab="Pr(Death)", xlab="Dose",
main="Fitted Probabilities")

coefs <- coef(glm.fit)
curve(invlogit(coefs[1] + coefs[2]*x),
lwd=2, col=2, add=TRUE)
points(unique(frog.data$dose), emp.probs, pch=15, cex=1.5)
@
\end{center}

\item The model I will use to do the analysis in a Bayesian framework is shown below. 
\begin{align*}
y_{ij} &\sim Bern(logit^{-1}(\alpha + \beta x_j)) \\
\alpha &\propto 1 \\
\beta &\propto 1
\end{align*}

\item The model code is shown below.
<<stanmodel2, echo=TRUE, eval=FALSE>>=
data {
  int<lower=0> N;
  vector[N] x;
  int<lower=0,upper=1> y[N];
}
parameters {
  real alpha;
  real beta;
}
model {
  y ~ bernoulli_logit(alpha + beta * x);
}
@


<<stanfit, echo=TRUE, message=FALSE, cache=TRUE, results='hide'>>=
require(rstan)
set.seed(23)
data <- with(frog.data, list(y = death, x = dose, N = length(death)))

model2 <- stan_model(file = "~/Documents/Stat532/exams/exam2/model2.stan", 
                     model_name = "model2")
samp2 <- sampling(model2, chains = 4, iter = 2000, data = data)
@

\item The posterior draws for the y-intercept ($\alpha$) and the dose effect ($\beta$) are shown below. The red line indicates the estimate from the above generalized linear model.

<<compare, echo=FALSE, out.width="\\linewidth", fig.width=10>>=
par(mfrow=c(1,2))

alpha <- extract(samp2)$alpha

hist(alpha, freq = F, xlab=expression(alpha), nclass=30, main=expression(alpha))
abline(v=coefs[1], col="red", lwd=3)

beta <- extract(samp2)$beta
hist(beta, freq=F, xlab=expression(beta), nclass=30, main= expression(beta))
abline(v=coefs[2], col="red", lwd=3)
@

\item Discuss and explain

\item The plots of the posterior draws for $\alpha$ and $\beta$ are shown below for the model with standardized dose as the predictor variable. The posterior draws for $\alpha$ have changed, but this is expected because $\alpha$ is now the log odds of death at the average dose. Standardizing dose does not appear to change inference about the effect of dose on the log odds of survival. Also, the efficiency of the sampler has not changed noticeably.

<<standardizedstanfit, echo=FALSE, message=FALSE, cache=TRUE, results='hide'>>=
set.seed(58)
dose.standard <- with(frog.data, (dose - mean(dose))/sd(dose))

std.data <- with(frog.data, list(y = death, x = dose.standard, N = length(death)))

std.model2 <- stan_model(file = "~/Documents/Stat532/exams/exam2/model2.stan", 
                     model_name = "std.model2")
std.samp2 <- sampling(std.model2, chains = 4, iter = 2000, data = std.data)
@

<<stdcompare, echo=FALSE, out.width="\\linewidth", fig.width=10>>=
par(mfrow=c(1,2))

alpha <- extract(std.samp2)$alpha

hist(alpha, freq = F, xlab=expression(alpha), nclass=30, main=expression(alpha))
abline(v=coefs[1], col="red", lwd=3)

beta <- extract(std.samp2)$beta
hist(beta, freq=F, xlab=expression(beta), nclass=30, main= expression(beta))
abline(v=coefs[2], col="red", lwd=3)
@

\item 

<<dcauchy, echo=FALSE>>=
dcauchy(1, 0, 10)
@



\end{enumerate}

\end{enumerate}

\end{document}